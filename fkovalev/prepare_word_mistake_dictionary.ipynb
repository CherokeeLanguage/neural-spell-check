{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "from booking_mtlib.normalize import normalize\n",
    "from booking_mtlib.parallelize import multiprocess\n",
    "from booking_mtlib.sentence_tokenizer import SentenceTokenizer\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok = ReTokenizer()\n",
    "words = set([tok.tokenize(l.strip('\\n').decode('utf-8').lower())[0] for l in open('/usr/share/dict/words')])\n",
    "len(words)\n",
    "import itertools\n",
    "import random\n",
    "verbs = set(itertools.chain.from_iterable((tok.tokenize(w)[0] for w in l.strip('\\n').decode('utf-8').lower().split(',') if w) for l in open('verb.txt')))\n",
    "len(verbs)\n",
    "all_words = verbs | words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "wg_f = open('word_splits.txt', 'w')\n",
    "\n",
    "for i, w in enumerate(all_words):\n",
    "    valid_splits = set()\n",
    "    splits = [(w[:i], w[i:]) for i in range(1, len(w))]\n",
    "    for splt in splits:\n",
    "        if splt[0] in all_words and splt[1] in all_words:\n",
    "            valid_splits.add(u\" \".join(splt))\n",
    "    if valid_splits:\n",
    "        wg_f.write(\"\\t\".join((w, u\",\".join(valid_splits))).encode('utf-8'))\n",
    "        wg_f.write(\"\\n\")\n",
    "wg_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_possible_replaces = defaultdict(set)\n",
    "f = open('word_graph.txt')\n",
    "for l in f:\n",
    "    w, s = l.strip('\\n').decode('utf-8').split(u'\\t')\n",
    "    for ws in s.split(u','):\n",
    "        if ws != w:\n",
    "            all_possible_replaces[w].add(ws)\n",
    "\n",
    "f = open('word_splits.txt')\n",
    "for l in f:\n",
    "    w, s = l.strip('\\n').decode('utf-8').split(u'\\t')\n",
    "    for ws in s.split(u','):\n",
    "        if ws != w:\n",
    "            all_possible_replaces[w].add(ws)\n",
    "\n",
    "f = open('verb.txt')\n",
    "for l in f:\n",
    "    ws = set(tok.tokenize(w)[0] for w in l.strip('\\n').decode('utf-8').lower().split(',') if w)\n",
    "    for w1 in ws:\n",
    "         for w2 in ws:\n",
    "            if w1 != w2:\n",
    "                all_possible_replaces[w1].add(w2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
